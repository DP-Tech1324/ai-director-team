CONFIG = {
    "llm_config": {
        "model": "phi3",  # or "gemma3" if you prefer
        "base_url": "http://localhost:11434/v1",
        "api_key": "ollama-local",  # placeholder
        "api_type": "openai"
    }
}